---
title: "cleaning_data"
output: pdf_document
date: "`r Sys.Date()`"
---

```{r}
# load libraries
library(tidyverse)
library(ggplot2)
library(forecast)
library(lubridate)
library(dplyr)
library(astsa)
```

```{r} 
# load data
prices_data <- read_csv("DailyPrices_ICCO.csv", show_col_types=FALSE)
ghana_data <- read_csv("Ghana_data.csv", show_col_types=FALSE)
```

```{r}
clean_ghana <- ghana_data %>%
  mutate(DATE = ymd(DATE)) %>%
  distinct(DATE, .keep_all = TRUE) %>%
  filter(between(DATE, ymd("1994-10-03"), ymd("2024-11-28"))) %>%
  arrange(DATE)
clean_ghana
```
```{r}
total_duplicates <- sum(duplicated(clean_ghana))
total_duplicates
```
```{r}
final_ghana <- clean_ghana %>%
  select(DATE, PRCP, TAVG)

final_ghana
```



```{r}
# Load necessary library

# Find rows with duplicated dates
duplicate_dates <- prices_data %>%
  group_by(Date) %>%        # Group by the "Date" column
  filter(n() > 1) %>%       # Keep dates that appear more than once
  arrange(Date) %>%         # Sort by date for clarity
  ungroup()

# Modify the column name from ICCO daily price (US$/tonne) to Price
prices_data <- rename(
  prices_data, 
  Price=`ICCO daily price (US$/tonne)`
)

# View the results
print(duplicate_dates)

```


```{r}
data <- prices_data %>%
  mutate(Date = ymd(Date)) %>%
  arrange(Date) %>%
  mutate(
    next_price = lead(Price),
    next_date = lead(Date),
    days_diff = as.numeric(next_date - Date),
    price_diff = next_price - Price
  ) %>%
  filter(
    days_diff == 1,          # Strictly consecutive days
    price_diff > 100,        # Price increase > $100
    !is.na(price_diff)       # Remove NA from the last row
  ) %>%
  select(Date, Price, next_date, next_price, price_diff)

# Print results
if (nrow(data) == 0) {
  message("No rows met the criteria.")
} else {
  print(data)
}

```


```{r}
clean_price <- prices_data %>%
  distinct() %>%
  filter(!(Date == "30/01/2024" & `Price` == 10676.42)) %>%
  filter(!(Date == "31/01/2024" & `Price` == 10888.05))
```

```{r}
# Load necessary library

# Find rows with duplicated dates
duplicate_dates <- clean_price %>%
  group_by(Date) %>%        # Group by the "Date" column
  filter(n() > 1) %>%       # Keep dates that appear more than once
  arrange(Date) %>%         # Sort by date for clarity
  ungroup()

print(duplicate_dates)

```


```{r}
final_price <- clean_price %>%
  select(Date, `Price`)

```
```{r}
final_price <- final_price %>%
  mutate(Date = as.Date(Date, format = "%d/%m/%Y"))

# Ensure DATE column is already Date type (if not, convert)
final_ghana <- final_ghana %>%
  mutate(DATE = as.Date(DATE))

```



```{r}
combined_data <- final_price %>%
  inner_join(final_ghana, by = c("Date" = "DATE"))

combined_data
```
```{r}
sorted_data <- combined_data %>%
  mutate(Date = ymd(Date)) %>%       # Convert to proper date format
  arrange(Date) %>%                 # Sort by ascending date
  select(Date, everything())        # Ensure date column comes first

# View sorted data
sorted_data

```
```{r}
sorted_data <- sorted_data %>%
  rename(
    date = Date,
    daily_price = `Price`,  
    precipitation = PRCP,                               
    avg_temperature = TAVG                             
  )

```

```{r}
tail(sorted_data)
```
```{r}
# make new data that converts the sorted_data$daily_price into monthly data by taking the average of the month
monthly_data <- sorted_data %>%
  mutate(year_month = format(date, "%Y-%m")) %>%
  group_by(year_month) %>%
  summarise(
    daily_price = mean(daily_price),
    precipitation = mean(precipitation),
    avg_temperature = mean(avg_temperature)
  ) %>%
  arrange(year_month)
```

```{r}
# Split the data into training and testing data

# Data from 2024 or before is for training
train_data <- monthly_data %>% 
  filter(as.integer(substr(year_month, 1, 4)) <= 2023)

test_data <- monthly_data %>%
  filter(as.integer(substr(year_month, 1, 4)) > 2023)
```


```{r}
# This code is here so I dont't have to manually replace all of the following code's monthly_data entries to train_data
monthly_data <- train_data
```

```{r}
prices <- ts(monthly_data$daily_price, frequency = 12)
plot(prices)
# months since 1994-10
ets_aaa <- ets(prices, model = "AAA")
plot(ets_aaa)
```

From this we know that there exists seasonality, indicating SARIMA is the appropriate approach.
```{r}
# 
acf(prices)
pacf(prices)
acf(diff(prices))
pacf(diff(prices))
```
From first differencing, it suggests that p,d,q = 1,1,1 is the appropriate model.

```{r}
auto.arima(prices)
```


# PAST WORK

```{r}
# Transform daily_price into ts object
dp_ts <- ts(sorted_data$daily_price, frequency = 12)
plot(sorted_data$date, sorted_data$daily_price)

# Plot ACF and PACF
acf(dp_ts, main = "ACF of daily_price", lag.max = 120)
pacf(dp_ts, main = "PACF of daily_price", lag.max = 120)
```

```{r}
# Take the first difference
diff_dp_ts <- diff(dp_ts)

# Plot ACF and PACF
acf(diff_dp_ts, 
    main = "ACF pf daily_price with one differencing", 
    lag.max = 120)
pacf(diff_dp_ts, 
     main = "PACF pf daily_price with one differencing", 
     lag.max = 120)
```


```{r}
# Take the second difference
diff_dp_ts2 <- diff(diff_dp_ts)

# Plot ACF and PACF
acf(diff_dp_ts2, 
    main = "ACF pf daily_price with two differencing", 
    lag.max = 120)
pacf(diff_dp_ts2, 
     main = "PACF pf daily_price with two differencing", 
     lag.max = 120)
```


```{r}
fit_021 <- sarima(dp_ts, 0, 2, 1)
fit_021
```

```{r}
fit_120 <- sarima(dp_ts, 1, 2, 0)
fit_120
```

```{r}
fit_121 <- sarima(dp_ts, 1, 2, 1)
fit_121
```

```{r}
# Implement log transform (should I do this...?)
dp_ts_log <- log(dp_ts)

fit_121_log <- sarima(dp_ts_log, 1, 2, 1)
fit_121_log
```

## Until this point, we just did some ts analysis, ACF, PACF to infer some MA, AR dependency. and potential GARCH model stuff.


```{r}
# ?????????? can we make this somewhat transferable to A2 material
mvspec(dp_ts,
       spans = c(3, 3),
       taper = 0.1, 
       log = "no",     
       main = "Spectral Analysis with mvspec"
) 
```

## MVSPEC: whats the purpose of this? What kind of information should we get out of spectral analysis?



```{r}
# Downloading S&P GSCI. The Purpose of downloading S&P GSCI is to understand the trend of how investors look at the commodities. While it is true that there are tons of other stuff in S&P, I think we should not use Cocoa index directly as causation is not very sure...

# read S&P GSCI
library(readr)
sp_gsci <- read.csv2(
  "S&P.csv",
  header = TRUE,
  stringsAsFactors = FALSE
)
```

```{r}
clean_sp <- sp_gsci %>%
  rename(
    date = "Issue.Date",
    gsci      = "S.P.GSCI"
  ) %>%
  mutate(date = mdy(date)) %>%
  distinct(date, .keep_all = TRUE) %>%
  arrange(date)
```

```{r}
summary(clean_sp)
```

```{r}
filtered_sp <- clean_sp %>%
  filter(
    date >= "2015-02-27",
    date <= "2024-11-28"
  )

filtered_sorted <- sorted_data %>%
  filter(
    date >= "2015-02-27",
    date <= "2024-11-28"
  )

# 1) Plot the first series (red) with normal axes
plot(
  x    = filtered_sp$date,
  y    = filtered_sp$gsci,
  type = "l",
  col  = "red",
  xlab = "Date",
  ylab = "S&P GSCI",
  xlim = c(as.Date("2015-02-27"), as.Date("2024-11-28"))
)

# 2) Add the second series (blue) on top, with a new y-axis
par(new = TRUE)  # tell R we will draw on top of the existing plot
plot(
  x    = filtered_sorted$date,
  y    = filtered_sorted$daily_price,
  type = "l",
  col  = "blue",
  axes = FALSE,         # don't draw a new x or y axis
  xlab = "",            # don't overwrite existing x label
  ylab = "",            # or y label
  xlim = c(as.Date("2015-02-27"), as.Date("2024-11-28"))
)

# 3) Draw a new axis on the right side, and label it appropriately
axis(side = 4)                # y-axis on the right
mtext("Daily Price", side = 4, line = 3)  # label the new axis
```

```{r}
inflation <- read.csv2(
  "inflation.csv",
  header = TRUE,
  stringsAsFactors = FALSE,
)

inflation <- inflation %>%
  separate(
    col  = "Date.Inflation.....",  
    into = c("Date", "Inflation"),  
    sep  = ","                      
  )
```

```{r}
clean_inflation <- inflation %>%
  rename(
    date = "Date",
    inflation = "Inflation"
  ) %>%
  mutate(date = ymd(date)) %>%
  distinct(date, .keep_all = TRUE) %>%
  arrange(date)

filtered_inflation <- clean_inflation %>%
  filter(
    date >= "2015-02-27",
    date <= "2024-11-28"
  )

summary(filtered_inflation)
```

```{r}
# 1) Plot the first series (red) with normal axes
plot(
  x    = filtered_inflation$date,
  y    = filtered_inflation$inflation,
  type = "l",
  col  = "red",
  xlab = "Date",
  ylab = "Inflation",
  xlim = c(as.Date("2015-02-27"), as.Date("2024-11-28"))
)

# 2) Add the second series (blue) on top, with a new y-axis
par(new = TRUE)  # tell R we will draw on top of the existing plot
plot(
  x    = filtered_sorted$date,
  y    = filtered_sorted$daily_price,
  type = "l",
  col  = "blue",
  axes = FALSE,         # don't draw a new x or y axis
  xlab = "",            # don't overwrite existing x label
  ylab = "",            # or y label
  xlim = c(as.Date("2015-02-27"), as.Date("2024-11-28"))
)

# 3) Draw a new axis on the right side, and label it appropriately
axis(side = 4)                # y-axis on the right
mtext("Daily Price", side = 4, line = 3)  # label the new axis
```



```{r}
# Shift 'date'
filtered_inflation <- filtered_inflation %>%
  mutate(date = date %m+% months(16))

filtered_sp <- filtered_sp %>%
  mutate(date = date %m+% months(24))

# Adjust the numeric value for estimation
filtered_inflation <- filtered_inflation %>%
  mutate(inflation = as.numeric(inflation) * 200)

filtered_sp$gsci <- filtered_sp$gsci * 12
```

```{r}
# 1) Aggregate daily_price to monthly average
df_blue_monthly <- filtered_sorted %>%
  mutate(month_start = floor_date(date, "month")) %>%
  group_by(month_start) %>%
  summarize(
    monthly_avg_price = mean(daily_price, na.rm = TRUE)
  ) %>%
  ungroup() %>%
  rename(date = month_start)  # rename for consistency

head(df_blue_monthly)
```

```{r}
# Split the prices into train/test
train_prices <- df_blue_monthly %>%
  filter(date < ymd("2023-12-01"))

test_prices <- df_blue_monthly %>%
  filter(date >= ymd("2023-12-01"))

#Check the head and tail to make sure the data is correctly splitted
tail(train_prices)
head(test_prices)
```



```{r}
df_gsci_monthly <- filtered_sp %>%
  mutate(month_start = floor_date(date, "month")) %>%
  group_by(month_start) %>%
  summarize(
    monthly_gsci = mean(gsci, na.rm = TRUE)
  ) %>%
  ungroup() %>%
  rename(date = month_start)
```

```{r}
# Split the gsci data into train/test
train_gsci <- df_gsci_monthly %>%
  filter(date < ymd("2023-12-01"))

test_gsci <- df_gsci_monthly %>%
  filter(date >= ymd("2023-12-01"))

#Check the head and tail to make sure the data is correctly splitted
tail(train_gsci)
head(test_gsci)
```

```{r}
df_monthly_train <- train_prices %>%
  left_join(filtered_inflation, by = "date") %>%
  left_join(train_gsci, by = "date")

df_monthly_test <- test_prices %>%
  left_join(filtered_inflation, by = "date") %>%
  left_join(test_gsci, by = "date")

head(df_monthly_test)
```

```{r}
df_monthly_train <- df_monthly_train %>%
  mutate(gsci_div2 = monthly_gsci / 2)

df_monthly_test <- df_monthly_test %>%
  mutate(gsci_div2 = monthly_gsci / 2)
```

```{r}
start_year  <- year(df_monthly_train$date[1])
start_month <- month(df_monthly_train$date[1])

ts_price <- ts(
  data = df_monthly_train$monthly_avg_price,
  start = c(start_year, start_month),
  frequency = 12
)

ts_inflation <- ts(
  data = df_monthly_train$inflation,
  start = c(start_year, start_month),
  frequency = 12
)

ts_gsci_div2 <- ts(
  data = df_monthly_train$gsci_div2,
  start = c(start_year, start_month),
  frequency = 12
)

# Combine them into a matrix for xreg:
xreg_all <- cbind(ts_inflation, ts_gsci_div2)
```

```{r}
fit_arima <- auto.arima(
  y    = ts_price,
  xreg = xreg_all,     # inflation and (gsci/2)
  stepwise      = FALSE,
  approximation = FALSE
)

summary(fit_arima)
```

```{r}
# predict future prices
start_year  <- year(df_monthly_test$date[1])
start_month <- month(df_monthly_test$date[1])

ts_price <- ts(
  data = df_monthly_test$monthly_avg_price,
  start = c(start_year, start_month),
  frequency = 12
)

ts_inflation <- ts(
  data = df_monthly_test$inflation,
  start = c(start_year, start_month),
  frequency = 12
)

ts_gsci_div2 <- ts(
  data = df_monthly_test$gsci_div2,
  start = c(start_year, start_month),
  frequency = 12
)

# Combine them into a matrix for xreg:
xreg_test <- cbind(ts_inflation, ts_gsci_div2)



forecast_arima <- forecast(fit_arima, xreg = xreg_test)

forecast_arima$mean
```

```{r}
# Calculate the error. Get the MSE
actual_prices <- df_monthly_test$monthly_avg_price

predicted_prices <- as.numeric(forecast_arima$mean)

mse <- mean((actual_prices - predicted_prices)^2)

mse

df_monthly_test
```


```{r}
# Extract fitted values from the model:
fitted_vals <- fit_arima$fitted

# Convert to numeric
fitted_vals <- as.numeric(fitted_vals)

# Actual
actual_vals <- df_monthly_train$monthly_avg_price
```

```{r}
mae <- mean(abs(actual_vals - fitted_vals), na.rm = TRUE)
mse <- mean((actual_vals - fitted_vals)^2, na.rm = TRUE)

cat("MAE =", mae, "\n")
cat("MSE =", mse, "\n")
```

```{r}
plot(
  x    = df_monthly_train$date,
  y    = df_monthly_train$monthly_avg_price,
  type = "l",
  col  = "blue",
  xlab = "Date",
  ylab = "Price",
  main = "Actual vs Model (Base R)"
)

lines(
  x   = df_monthly_train$date,
  y   = fitted_vals,
  col = "red",
  lty = 2
)

legend(
  "topleft",
  legend = c("Actual", "Model"),
  col    = c("blue", "red"),
  lty    = c(1, 2),
  cex    = 0.9
)
```




